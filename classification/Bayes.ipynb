{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 朴素贝叶斯分类器\n",
    "## 使用Python进行文本分类\n",
    "对于网上留言，我们使用Python来鉴别它是否是侮辱性言论。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    \"\"\"\n",
    "    创建了一些实验样本\n",
    "    \"\"\"\n",
    "    dataset=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], \n",
    "             ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], \n",
    "             ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], \n",
    "             ['stop', 'posting', 'stupid', 'worthless', 'garbage'], \n",
    "             ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], \n",
    "             ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classes = [0, 1, 0, 1, 0, 1]  #1 is abusive(骂人的，滥用的), 0 not\n",
    "    return dataset, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(dataset):\n",
    "    \"\"\"\n",
    "    创建一个包含在所有文档中出现的不重复词的列表\n",
    "    \"\"\"\n",
    "    vocabulary = set()\n",
    "    for sample in dataset:\n",
    "        for s in sample:\n",
    "            vocabulary.add(s)\n",
    "    return list(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorization(vocabulary, sample):\n",
    "    \"\"\"\n",
    "    返回文本向量，0表示单词未出现，1表示出现\n",
    "    \"\"\"\n",
    "    vector = np.zeros(len(vocabulary))\n",
    "    for s in sample:\n",
    "        if s in vocabulary:\n",
    "            vector[vocabulary.index(s)] = 1  # 也可以是`+= 1` 表示出现次数\n",
    "        else: print(\"the word: %s is not in my Vocabulary!\" % s)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
       " ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
       " ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
       " ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
       " ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
       " ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 1, 0, 1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['buying', 'worthless', 'dog', 'ate', 'love', 'park', 'to', 'maybe', 'how', 'please', 'flea', 'my', 'has', 'help', 'is', 'quit', 'food', 'cute', 'stop', 'posting', 'so', 'not', 'mr', 'dalmation', 'problems', 'take', 'licks', 'stupid', 'garbage', 'I', 'steak', 'him']\n"
     ]
    }
   ],
   "source": [
    "vocabulary = create_vocabulary(dataset)\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(vectorization(vocabulary, dataset[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(c_i | w) = \\frac{p(w|c_i)p(c_i)}{p(w)}$$\n",
    "其中，$w$表示文本向量，$c_i$表示分类结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 朴素贝叶斯分类器算法\n",
    "```\n",
    "计算每个类别中的文本的数目\n",
    "对每篇训练文档：\n",
    "    对每个类别：\n",
    "        如果词条出现在文档中=>增加该词条的计数值\n",
    "        增加所有词条的计数值\n",
    "    对每个类别：\n",
    "        对每个词条：\n",
    "            将该词条的数目除以总词条的数目得到条件概率\n",
    "    返回每个类别的条件概率\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, classes):\n",
    "    n_sample = len(dataset)\n",
    "    n_word = len(dataset[0])\n",
    "    p_abusive = sum(classes) / float(n_sample)\n",
    "    # 防止0有些单词的条件概率为0，导致相乘后的结果等于0\n",
    "    p0_word = np.ones(n_word)\n",
    "    p1_word = np.ones(n_word)\n",
    "    p0_sum = 2.0\n",
    "    p1_sum = 2.0\n",
    "    for i in range(n_sample):\n",
    "        if classes[i] == 1:\n",
    "            p1_word += dataset[i]\n",
    "            p1_sum += sum(dataset[i])\n",
    "        else:\n",
    "            p0_word += dataset[i]\n",
    "            p0_sum += sum(dataset[i])\n",
    "    # change to log()\n",
    "    p1_vector = np.log(p1_word / p1_sum)\n",
    "    p0_vector = np.log(p0_word / p0_sum)\n",
    "    return p0_vector, p1_vector, p_abusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_dataset = []\n",
    "for sample in dataset:\n",
    "    vectorized_dataset.append(vectorization(vocabulary, sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,\n",
       "         1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  1.,  0.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,\n",
       "         0.,  1.,  0.,  0.,  0.,  1.]),\n",
       " array([ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "         0.,  1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,\n",
       "         0.,  0.,  0.,  1.,  0.,  1.]),\n",
       " array([ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  1.,  1.,  0.,  0.,  0.]),\n",
       " array([ 0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  1.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
       "         1.,  0.,  0.,  0.,  1.,  1.]),\n",
       " array([ 1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  1.,  0.,  0.,  0.,  0.])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0_v, p1_v, p_ab = train(vectorized_dataset, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-3.25809654, -3.25809654, -2.56494936, -2.56494936, -2.56494936,\n",
       "        -3.25809654, -2.56494936, -3.25809654, -2.56494936, -2.56494936,\n",
       "        -2.56494936, -1.87180218, -2.56494936, -2.56494936, -2.56494936,\n",
       "        -3.25809654, -3.25809654, -2.56494936, -2.56494936, -3.25809654,\n",
       "        -2.56494936, -3.25809654, -2.56494936, -2.56494936, -2.56494936,\n",
       "        -3.25809654, -2.56494936, -3.25809654, -3.25809654, -2.56494936,\n",
       "        -2.56494936, -2.15948425]),\n",
       " array([-2.35137526, -1.94591015, -1.94591015, -3.04452244, -3.04452244,\n",
       "        -2.35137526, -2.35137526, -2.35137526, -3.04452244, -3.04452244,\n",
       "        -3.04452244, -3.04452244, -3.04452244, -3.04452244, -3.04452244,\n",
       "        -2.35137526, -2.35137526, -3.04452244, -2.35137526, -2.35137526,\n",
       "        -3.04452244, -2.35137526, -3.04452244, -3.04452244, -3.04452244,\n",
       "        -2.35137526, -3.04452244, -1.65822808, -2.35137526, -3.04452244,\n",
       "        -3.04452244, -2.35137526]),\n",
       " 0.5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0_v, p1_v, p_ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(sample, p0_v, p1_v, p_ab):\n",
    "    p1 = sum(sample * p1_v) + np.log(p_ab)\n",
    "    p0 = sum(sample * p0_v) + np.log(1.0 - p_ab)\n",
    "    return int(p1 > p0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    dataset, classes = load_dataset()\n",
    "    vocabulary = create_vocabulary(dataset)\n",
    "    samples = []\n",
    "    for sample in dataset:\n",
    "        samples.append(vectorization(vocabulary, sample))\n",
    "    p0_v, p1_1, p_ab = train(np.array(samples), np.array(classes))\n",
    "    test_sample = ['love', 'my', 'dalmation']\n",
    "    test_sample = np.array(vectorization(vocabulary, test_sample))\n",
    "    print('=>', classification(test_sample, p0_v, p1_v, p_ab))\n",
    "    test_sample = ['stupid', 'garbage']\n",
    "    test_sample = np.array(vectorization(vocabulary, test_sample))\n",
    "    print('=>', classification(test_sample, p0_v, p1_v, p_ab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> 0\n",
      "=> 1\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
