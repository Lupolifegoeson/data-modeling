{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic回归\n",
    "## sigmoid函数\n",
    "$$\\sigma (z) = \\frac 1{1+e^{-z}}$$\n",
    "### sigmoid函数的输入\n",
    "$$z = w_0x_0 + w_1x_1 + ... + w_nx_n = W^TX$$\n",
    "## 梯度下降算法\n",
    "$$w:=w+\\alpha \\nabla f(w)$$\n",
    "### 梯度下降伪代码\n",
    "```\n",
    "初始化回归系数w\n",
    "重复R次：\n",
    "    计算整个数据集的梯度\n",
    "    使用alpha*gradient更新回归系数的向量\n",
    "    返回回归系数\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    dataset, labels = [], []\n",
    "    with open('logistic-dataset.txt') as f:\n",
    "        for line in f.readlines():\n",
    "            line_list = line.strip().split()\n",
    "            dataset.append([1.0, float(line_list[0]), float(line_list[1])])\n",
    "            labels.append(int(line_list[2]))\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, labels = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(dataset, labels):\n",
    "    data_matrix = np.mat(dataset)\n",
    "    label_matrix = np.mat(labels).transpose()\n",
    "    m, n = data_matrix.shape\n",
    "    alpha = 0.001\n",
    "    epochs = 500\n",
    "    weights = np.ones((n, 1))\n",
    "    for k in range(epochs):  # heavy on matrix operations\n",
    "        h = sigmoid(data_matrix * weights)  # matrix multiple\n",
    "        error = (label_matrix - h)  # vector subtraction\n",
    "        weights = weights + alpha * data_matrix.transpose() * error  # matrix mult\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 4.12414349],\n",
       "        [ 0.48007329],\n",
       "        [-0.6168482 ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradientDescent(dataset, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机梯度下降算法\n",
    "梯度下降算法在每次更新回归系数时都需要遍历整个数据集，该方法在处理100个左右的数据集是尚可，但如果有数十亿的样本和成千上万的特征，那么该方法的计算复杂度就太高了。\n",
    "\n",
    "一种改进方法是一次仅用一个样本点来更新回归系数，该方法成为随机梯度下降算法。\n",
    "### 随机梯度下降伪代码\n",
    "```\n",
    "所有回归系数初始化为1\n",
    "对数据集中的每个样本\n",
    "    计算该样本的梯度\n",
    "    使用alpha*gradient更新回归系数值\n",
    "返回回归系数值\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochasticGradDes(data_matrix, labels, epochs=150):\n",
    "    m, n = data_matrix.shape\n",
    "    weights = np.ones(n)\n",
    "    for j in range(epochs):\n",
    "        data_index = list(range(m))\n",
    "        for i in range(m):\n",
    "            alpha = 4 / (1.0 + j + i) + 0.0001  # alpha decreases with iteration\n",
    "            random_index = int(np.random.uniform(0, len(data_index)))\n",
    "            h = sigmoid(sum(data_matrix[random_index] * weights))\n",
    "            error = labels[random_index] - h\n",
    "            weights = weights + alpha * error * data_matrix[random_index]\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 13.86193504,   1.36557302,  -1.87783856])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stochasticGradDes(np.array(dataset), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
