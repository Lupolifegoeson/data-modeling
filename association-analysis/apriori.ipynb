{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用Apriori算法进行关联分析\n",
    "## 基本概念\n",
    "关联分析既不是监督学习也不是非监督学习，所以在python的机器学习库`sklearn`中没有API。准确的说，关联分析是一种统计分析概率和条件概率的方法，分别对应两个统计目标——发现频繁项集和发现关联规则。\n",
    "## Apriori原理\n",
    "假设我们一共有4个商品：商品0，商品1，商品2，商品3。\n",
    "\n",
    "如下图所示，已知灰色部分{2,3}是非频繁项集，那么利用上面的知识，我们就可以知道{0,2,3}{1,2,3} {0,1,2,3}都是非频繁的。 也就是说，计算出{2,3}的支持度，知道它是非频繁的之后，就不需要再计算{0,2,3} {1,2,3} {0,1,2,3}的支持度，因为我们知道这些集合不会满足我们的要求。 使用该原理就可以避免项集数目的指数增长，从而在合理的时间内计算出频繁项集。\n",
    "![infrequent](infrequent.png)\n",
    "\n",
    "首先需要找到频繁项集，然后才能发现关联规则。\n",
    "\n",
    "Apriori 算法是发现频繁项集的一种方法。 Apriori算法的两个输入参数分别是最小支持度和数据集。 该算法首先会生成所有单个物品的项集列表。 接着扫描交易记录来查看哪些项集满足最小支持度要求，那些不满足最小支持度要求的集合会被去掉。 然后对生下来的集合进行组合以生成包含两个元素的项集。 接下来再重新扫描交易记录，去掉不满足最小支持度的项集。 该过程重复进行直到所有项集被去掉。\n",
    "### 生成候选项集\n",
    "```\n",
    "对数据集中的每一条交易记录transection\n",
    "对每个候选项集candidate\n",
    "    检查一下candidate是否是transection的子集:\n",
    "        如果是则增加candidate的计数值\n",
    "对每个候选项集\n",
    "    如果其支持度不低于最小值，则保留该项集\n",
    "    返回所有频繁项集列表\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 我们首先创建一个数据集。数据集包含5中商品1、2、3、4、5，待会就通过Apriori算法发现频繁项集和关联规则。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    \"\"\"\n",
    "    创建一个数据集\n",
    "    \"\"\"\n",
    "    return [[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], [2, 5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], [2, 5]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 通过生成的dataset初步生成只有一项的候选项集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_candidates1(dataset):\n",
    "    candidates1 = []  # 只有一项的候选项集\n",
    "    for transection in dataset:\n",
    "        for item in transection:\n",
    "            if [item] not in candidates1:\n",
    "                candidates1.append([item])\n",
    "    candidates1.sort()\n",
    "    return list(map(frozenset, candidates1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[frozenset({1}),\n",
       " frozenset({2}),\n",
       " frozenset({3}),\n",
       " frozenset({4}),\n",
       " frozenset({5})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates1 = create_candidates1(dataset)\n",
    "candidates1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 选择支持度大于最小支持度(min_support)的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_dataset(dataset, candidates_k, min_support=0.5):\n",
    "    candidate_cnt = {}  # 统计初步建立的候选项集中，项的出现次数\n",
    "    for transection in dataset:\n",
    "        for candidate in candidates_k:\n",
    "            if candidate.issubset(transection):\n",
    "                if candidate not in candidate_cnt:\n",
    "                    candidate_cnt[candidate] = 1\n",
    "                else:\n",
    "                    candidate_cnt[candidate] += 1\n",
    "    \n",
    "    len_dataset = float(len(dataset))\n",
    "    checked_candidates = []\n",
    "    supports = {}\n",
    "    for key in candidate_cnt:\n",
    "        support = candidate_cnt[key] / len_dataset\n",
    "        if support >= min_support:\n",
    "            checked_candidates.append(key)\n",
    "        supports[key] = support\n",
    "    return checked_candidates, supports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 4]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[frozenset({1}), frozenset({3}), frozenset({2}), frozenset({5})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checked_candidates, supports = scan_dataset(dataset, candidates1)\n",
    "checked_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{frozenset({1}): 0.5,\n",
       " frozenset({3}): 0.75,\n",
       " frozenset({4}): 0.25,\n",
       " frozenset({2}): 0.75,\n",
       " frozenset({5}): 0.75}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述候选集中，由于候选项集{4}的支持度只有0.25<阈值0.5，故被删除"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 组织完整的Apriori算法\n",
    "Apriori算法伪代码：\n",
    "```\n",
    "当集合中项的个数大于0时：\n",
    "    构建一个k个项组成的候选项集的列表\n",
    "    检查数据已确认每个项集都是频繁的\n",
    "    保留频繁项集并构建k+1项组成的候选项集的列表\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori_gen(checked_candidates_k, k):\n",
    "    \"\"\"\n",
    "    生成不重复的下一级候选项集\n",
    "    \"\"\"\n",
    "    next_candidates = []\n",
    "    for i in range(len(checked_candidates_k) - 1):\n",
    "        for j in range(i + 1, len(checked_candidates_k)):\n",
    "            candidate1_pre = list(checked_candidates_k[i])[:k-2]\n",
    "            candidate2_pre = list(checked_candidates_k[j])[:k-2]\n",
    "            candidate1_pre.sort()\n",
    "            candidate2_pre.sort()\n",
    "            if candidate1_pre == candidate2_pre:\n",
    "                next_candidates.append(checked_candidates_k[i] | checked_candidates_k[j])\n",
    "    return next_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori(dataset, min_support=0.5):\n",
    "    '''一个项的频繁项集'''\n",
    "    candidates1 = create_candidates1(dataset)\n",
    "    checked_candidates1, supports = scan_dataset(dataset, candidates1, min_support)\n",
    "    \n",
    "    '''迭代生成最终频繁项集'''\n",
    "    rs_candidates = [checked_candidates1]\n",
    "    k = 2\n",
    "    while True:\n",
    "        candidates_k = apriori_gen(rs_candidates[k-2], k)\n",
    "#         print(candidates_k)\n",
    "        checked_candidates_k, supports_k = scan_dataset(dataset, candidates_k, min_support)\n",
    "        supports.update(supports_k)\n",
    "        rs_candidates.append(checked_candidates_k)\n",
    "        k += 1\n",
    "        if not checked_candidates_k:\n",
    "            break\n",
    "    return rs_candidates, supports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[frozenset({1}), frozenset({3}), frozenset({2}), frozenset({5})],\n",
       " [frozenset({1, 3}), frozenset({2, 3}), frozenset({3, 5}), frozenset({2, 5})],\n",
       " [frozenset({2, 3, 5})],\n",
       " []]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_candidates, supports = apriori(dataset)\n",
    "rs_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{frozenset({1}): 0.5,\n",
       " frozenset({3}): 0.75,\n",
       " frozenset({4}): 0.25,\n",
       " frozenset({2}): 0.75,\n",
       " frozenset({5}): 0.75,\n",
       " frozenset({1, 3}): 0.5,\n",
       " frozenset({2, 3}): 0.5,\n",
       " frozenset({3, 5}): 0.5,\n",
       " frozenset({2, 5}): 0.75,\n",
       " frozenset({1, 2}): 0.25,\n",
       " frozenset({1, 5}): 0.25,\n",
       " frozenset({2, 3, 5}): 0.5}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从频繁项集中挖掘关联规则\n",
    "```\n",
    "遍历项数大于1的频繁项集：\n",
    "    如果频繁项集中项的个数大于2:\n",
    "        按下图所示的进行迭代计算\n",
    "    否则，频繁项集中项的个数等于2：\n",
    "        计算条件概率\n",
    "```\n",
    "![apachecn_association_rule_demo_1](apachecn_association_rule_demo_1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rules(rs_candidates, supports, min_conf=0.6):\n",
    "    rules = []\n",
    "    for i in range(1, len(rs_candidates)):\n",
    "        for freq_set in rs_candidates[i]:\n",
    "            freq_list = [frozenset([item]) for item in freq_set]\n",
    "            if i > 1:\n",
    "                rules_from_big_freq_set(freq_set, freq_list, supports, rules, min_conf)\n",
    "            else:\n",
    "                calc_conf(freq_set, freq_list, supports, rules, min_conf)\n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_conf(freq_set, freq_list, supports, rules, min_conf=0.6):\n",
    "    checked_freq = []\n",
    "    for related in freq_list:\n",
    "        relating = freq_set - related\n",
    "        conf = supports[freq_set] / supports[freq_set - related]\n",
    "        if conf >= min_conf:\n",
    "            print(relating, '-->', related, 'conf: ', conf)\n",
    "            rules.append((relating, related, conf))\n",
    "            checked_freq.append(related)\n",
    "    return checked_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rules_from_big_freq_set(freq_set, freq_list, supports, rules, min_conf=0.6):\n",
    "    \"\"\"\n",
    "    freq_set: 项数相同的频繁项集的列表\n",
    "    freq_list: freq_set中的项的列表，一开始是一个项的列表，随迭代增加\n",
    "    \"\"\"\n",
    "    if len(freq_set) > (len(freq_list[0]) + 1):\n",
    "        next_freq_list = apriori_gen(freq_list, len(freq_list[0]) + 1)\n",
    "        next_freq_list = calc_conf(freq_set, next_freq_list, supports, rules, min_conf)\n",
    "        if len(next_freq_list) > 1:\n",
    "            rules_from_big_freq_set(freq_set, next_freq_list, supports, rules, min_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({3}) --> frozenset({1}) conf:  0.6666666666666666\n",
      "frozenset({1}) --> frozenset({3}) conf:  1.0\n",
      "frozenset({3}) --> frozenset({2}) conf:  0.6666666666666666\n",
      "frozenset({2}) --> frozenset({3}) conf:  0.6666666666666666\n",
      "frozenset({5}) --> frozenset({3}) conf:  0.6666666666666666\n",
      "frozenset({3}) --> frozenset({5}) conf:  0.6666666666666666\n",
      "frozenset({5}) --> frozenset({2}) conf:  1.0\n",
      "frozenset({2}) --> frozenset({5}) conf:  1.0\n",
      "frozenset({5}) --> frozenset({2, 3}) conf:  0.6666666666666666\n",
      "frozenset({3}) --> frozenset({2, 5}) conf:  0.6666666666666666\n",
      "frozenset({2}) --> frozenset({3, 5}) conf:  0.6666666666666666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(frozenset({3}), frozenset({1}), 0.6666666666666666),\n",
       " (frozenset({1}), frozenset({3}), 1.0),\n",
       " (frozenset({3}), frozenset({2}), 0.6666666666666666),\n",
       " (frozenset({2}), frozenset({3}), 0.6666666666666666),\n",
       " (frozenset({5}), frozenset({3}), 0.6666666666666666),\n",
       " (frozenset({3}), frozenset({5}), 0.6666666666666666),\n",
       " (frozenset({5}), frozenset({2}), 1.0),\n",
       " (frozenset({2}), frozenset({5}), 1.0),\n",
       " (frozenset({5}), frozenset({2, 3}), 0.6666666666666666),\n",
       " (frozenset({3}), frozenset({2, 5}), 0.6666666666666666),\n",
       " (frozenset({2}), frozenset({3, 5}), 0.6666666666666666)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules = generate_rules(rs_candidates, supports)\n",
    "rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
